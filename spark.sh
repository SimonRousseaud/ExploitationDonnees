#!/bin/bash
echo $HADOOP_HOME

# Set environment variables
if ! grep -q "SPARK_HOME" ~/.bashrc; then
    echo "export SPARK_HOME=/usr/local/spark" >> ~/.bashrc
    source ~/.bashrc;

fi

if ! grep -q "PATH.*SPARK_HOME" ~/.bashrc; then
    echo "export PATH=\$PATH:\$SPARK_HOME/bin" >> ~/.bashrc
fi

if ! grep -q "USER_PATH_CUSTOM" ~/.bashrc; then
    echo "export USER_PATH_CUSTOM=/home/ubuntu" >> ~/.bashrc
fi

if ! grep -q "HADOOP_HOME" ~/.bashrc; then
    echo "export HADOOP_HOME=/home/ubuntu/hadoop-3.3.4" >> ~/.bashrc
    source ~/.bashrc;
    echo "export HADOOP_INSTALL=\$HADOOP_HOME" >> ~/.bashrc
    echo "export HADOOP_MAPRED_HOME=\$HADOOP_HOME" >> ~/.bashrc
    echo "export HADOOP_COMMON_HOME=\$HADOOP_HOME" >> ~/.bashrc
    echo "export HADOOP_HDFS_HOME=\$HADOOP_HOME" >> ~/.bashrc
    echo "export YARN_HOME=\$HADOOP_HOME" >> ~/.bashrc
    echo "export HADOOP_COMMON_LIB_NATIVE_DIR=\$HADOOP_HOME/lib/native" >> ~/.bashrc
    echo "export PATH=\$PATH:\$HADOOP_HOME/sbin:\$HADOOP_HOME/bin" >> ~/.bashrc
    echo "export HADOOP_OPTS=\$HADOOP_HOME/lib/native" >> ~/.bashrc
fi
source ~/.bashrc;

# Update package manager
sudo apt update -y
echo "APT update OK "

# Install Java
sudo apt install openjdk-11-jdk git maven -y
echo "APT install OK "

# Install Hadoop

# wget -P "$USER_PATH_CUSTOM" https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
cd "$USER_PATH_CUSTOM" && tar -xvzf hadoop-3.3.4.tar.gz
cd hadoop-3.3.4/

echo "export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64" >> $HADOOP_HOME/etc/hadoop/hadoop-env.sh;
source $HADOOP_HOME/etc/hadoop/hadoop-env.sh;

# END NEW

# Install Spark
wget -P "$USER_PATH_CUSTOM" https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3-scala2.13.tgz;

echo "WGET OK ";

cd "$USER_PATH_CUSTOM" && tar xvf spark-3.3.1-bin-hadoop3-scala2.13.tgz;
echo "TAR OK ";
cd "$USER_PATH_CUSTOM" && sudo mkdir /usr/local/spark/ && sudo cp -r spark-3.3.1-bin-hadoop3-scala2.13/ /usr/local/spark/;
echo "MV OK";

cd "$USER_PATH_CUSTOM" && git clone https://github.com/SimonRousseaud/ExploitationDonnees.git ExploitationDonnees
echo "GIT CLONED OK "
# Deploy your application
cd "$USER_PATH_CUSTOM" && cd "ExploitationDonnees"  && mvn install
echo "MVN Install OK "

ip4=$(/sbin/ip -o -4 addr list ens33 | awk '{print $4}' | cut -d/ -f1)
echo "IP4 : "
echo "$ip4"

echo "STARTCAT"
echo "$(cat $USER_PATH_CUSTOM/ExploitationDonnees/config/core-site.xml)" > $USER_PATH_CUSTOM/etc/hadoop/core-site.xml
echo "$(cat $USER_PATH_CUSTOM/ExploitationDonnees/config/hdfs-site.xml)" > $USER_PATH_CUSTOM/etc/hadoop/hdfs-site.xml
echo "$(cat $USER_PATH_CUSTOM/ExploitationDonnees/config/mapred-site.xml)" > $USER_PATH_CUSTOM/etc/hadoop/mapred-site.xml
echo "$(cat $USER_PATH_CUSTOM/ExploitationDonnees/config/yarn-site.xml)" > $USER_PATH_CUSTOM/hadoop-3.3.4/etc/hadoop/yarn-site.xml
echo "STARTBASH"

cd "$USER_PATH_CUSTOM" && cd hadoop-3.3.4 && bash bin/hdfs namenode -format
cd "$USER_PATH_CUSTOM" && cd hadoop-3.3.4 && bash sbin/start-dfs.sh
cd "$USER_PATH_CUSTOM" && cd hadoop-3.3.4 && bash sbin/start-yarn.sh
start-worker.sh spark://"$ip4":7077

# Execute the application
spark-submit --class org.example.main --master spark://DESKTOP-2PBH57J:7077 --driver-library-path $HADOOP_HOME/lib/native --jars "$HADOOP_HOME/lib/*,$SPARK_HOME/jars/*" target/Projet-javaaaaa3-1.0-SNAPSHOT.jar
