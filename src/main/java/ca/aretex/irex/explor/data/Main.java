package ca.aretex.irex.explor.data;

import ca.aretex.irex.explor.data.naissances.beans.ActeNaissances;
import ca.aretex.irex.explor.data.naissances.functions.reader.ActeNaissancesReader;
import ca.aretex.irex.explor.data.naissances.functions.writer.ActeNaissancesWriter;
import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.fs.FileSystem;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.SparkSession;

import java.io.IOException;

@Slf4j
public class Main {
    public static void main(String[] args) throws InterruptedException, IOException {
        log.info("Hello world!");


        Config config = ConfigFactory.load("application.conf");
        String masterUrl = config.getString("app.master");
        String appName = config.getString("app.name");

        String inputPathStr = config.getString("app.path.input");
        String outputPathStr = config.getString("app.path.output");

        SparkSession sparkSession = SparkSession.builder()
                .master(masterUrl)
                .appName(appName)
                .getOrCreate();

        FileSystem hdfs = FileSystem.get(sparkSession.sparkContext().hadoopConfiguration());
        log.info("fileSystem got from sparkSession in the main : hdfs.getScheme = {}", hdfs.getScheme());


        ActeNaissancesReader reader = ActeNaissancesReader.builder()
                .sparkSession(sparkSession)
                .hdfs(hdfs)
                .inputPathStr(inputPathStr)
                .build();

        ActeNaissancesWriter<ActeNaissances> writer = new ActeNaissancesWriter<>(outputPathStr);


        Dataset<ActeNaissances> actenaissancesDataset = reader.get().cache();
        actenaissancesDataset.printSchema();
        actenaissancesDataset.show(5, false);

        log.info("nb actesnaissances = {}", actenaissancesDataset.count());

        writer.accept(actenaissancesDataset);

        log.info("done");
        //Thread.sleep(1000 * 60 * 10);

    }
}