package ca.aretex.irex.explor.data;

import ca.aretex.irex.explor.data.naissances.functions.processor.ActeNaissancesStreamProcessor;
import ca.aretex.irex.explor.data.naissances.functions.receiver.ActeNaissancesReceiver;
import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.fs.FileSystem;
import org.apache.spark.SparkConf;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import java.io.IOException;

@Slf4j
public class StreamingMain {
    public static void main(String[] args) throws InterruptedException, IOException {
        log.info("Hello world!");

        Config config = ConfigFactory.load("application.conf");
        String masterUrl = config.getString("app.master");
        String appName = config.getString("app.name");

        String inputPathStr = config.getString("app.path.input");
        String outputPathStr = config.getString("app.path.output");
        String checkPointStr = config.getString("app.path.checkpoint");

        log.info("\ninputPathStr={}\noutputPathStr={}\ncheckPointStr={}", inputPathStr, outputPathStr, checkPointStr);
        log.info("\nmasterUrl={}\nappName={}", masterUrl, appName);

        SparkConf sparkConf = new SparkConf().setMaster(masterUrl).setAppName(appName);
        JavaStreamingContext jsc = new JavaStreamingContext(
                sparkConf,
                new Duration(1000 * 10)
        );
        //jsc.checkpoint(checkPointStr);

        //SparkSession sparkSession = SparkSession.builder().master(masterUrl).appName(appName).getOrCreate();
        SparkSession sparkSession = SparkSession.builder().config(sparkConf).getOrCreate();

        FileSystem hdfs = FileSystem.get(sparkSession.sparkContext().hadoopConfiguration());
        log.info("fileSystem got from sparkSession in the main : hdfs.getScheme = {}", hdfs.getScheme());

//        JavaStreamingContext jsc = JavaStreamingContext.getOrCreate(
//                checkPointStr,
//                () -> {
//                    JavaStreamingContext javaStreamingContext = new JavaStreamingContext(
//                            JavaSparkContext.fromSparkContext(sparkSession.sparkContext()),
//                            new Duration(1000 * 10)
//                    );
//                    javaStreamingContext.checkpoint(checkPointStr);
//                    return javaStreamingContext;
//                },
//                sparkSession.sparkContext().hadoopConfiguration()
//        );
//        JavaStreamingContext jsc = new JavaStreamingContext(
//                JavaSparkContext.fromSparkContext(sparkSession.sparkContext()),
//                new Duration(1000 * 10)
//        );
//        jsc.checkpoint(checkPointStr);

        ActeNaissancesReceiver receiver = new ActeNaissancesReceiver(jsc, inputPathStr);
        ActeNaissancesStreamProcessor streamProcessor = new ActeNaissancesStreamProcessor(sparkSession, outputPathStr);

        receiver.get().foreachRDD(streamProcessor);

        jsc.start();
        jsc.awaitTerminationOrTimeout(1000 * 60 * 5);


    }
}