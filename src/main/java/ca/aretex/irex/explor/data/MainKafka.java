package ca.aretex.irex.explor.data;

import ca.aretex.irex.explor.data.naissances.beans.ActeNaissances;
import ca.aretex.irex.explor.data.naissances.functions.receiver.ActeNaissancesKafkaReceiver;
import ca.aretex.irex.explor.data.naissances.functions.writer.ActeNaissancesWriter;
import com.typesafe.config.Config;
import com.typesafe.config.ConfigFactory;
import org.apache.hadoop.fs.FileSystem;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.codehaus.jackson.map.deser.std.StringDeserializer;

import java.io.IOException;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class MainKafka {
    public static void main(String[] args) throws InterruptedException, IOException {


        Config config = ConfigFactory.load("application.conf");
        String masterUrl = config.getString("app.master");
        String appName = config.getString("app.name");

        String inputPathStr = config.getString("app.path.input");
        String outputPathStr = config.getString("app.path.output");
        String checkPointStr = config.getString("app.path.checkpoint");

        List<String> topics = config.getStringList("3il.kafka.list");


        SparkSession sparkSession = SparkSession.builder().master(masterUrl).appName(appName).getOrCreate();

        FileSystem hdfs = FileSystem.get(sparkSession.sparkContext().hadoopConfiguration());

        JavaStreamingContext javaStreamingContext = JavaStreamingContext.getOrCreate(
                checkPointStr,
                () -> {
                    JavaStreamingContext jsc = new JavaStreamingContext(
                            JavaSparkContext.fromSparkContext(sparkSession.sparkContext()),
                            new Duration(1000 * 10)
                    );
                    jsc.checkpoint(checkPointStr);


                    ActeNaissancesKafkaReceiver reciever = new ActeNaissancesKafkaReceiver(topics, jsc);
                    JavaDStream<ActeNaissances> mensualiteJavaDStream = reciever.get();

                    mensualiteJavaDStream.foreachRDD(
                            mensualiteJavaRDD -> {
                                Dataset<ActeNaissances> mensualiteDataset = SparkSession.active().createDataset(
                                        mensualiteJavaRDD.rdd(),
                                        Encoders.bean(ActeNaissances.class)
                                ).cache();

                                mensualiteDataset.printSchema();
                                mensualiteDataset.show(5, false);

                                ActeNaissancesWriter<ActeNaissances> writer = new ActeNaissancesWriter<>(outputPathStr);
                                writer.accept(mensualiteDataset);
                                mensualiteDataset.unpersist();
                            }
                    );

                    return jsc;
                },
                sparkSession.sparkContext().hadoopConfiguration()
        );

        javaStreamingContext.start();
        javaStreamingContext.awaitTerminationOrTimeout(1000 * 60 * 3);
    }
}
