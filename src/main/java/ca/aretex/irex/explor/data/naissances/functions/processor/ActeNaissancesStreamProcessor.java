package ca.aretex.irex.explor.data.naissances.functions.processor;

import ca.aretex.irex.explor.data.naissances.beans.ActeNaissances;
import ca.aretex.irex.explor.data.naissances.functions.writer.ActeNaissancesWriter;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.VoidFunction2;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Encoders;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.streaming.Time;

@Slf4j
@RequiredArgsConstructor
public class ActeNaissancesStreamProcessor implements VoidFunction2<JavaRDD<ActeNaissances>, Time> {
    private final SparkSession sparkSession;
    private final String outputPathStr;

    @Override
    public void call(JavaRDD<ActeNaissances> actenaissancesJavaRDD, Time time) throws Exception {
            long ts = System.currentTimeMillis();
        log.info("micro-batch time={} at stored in folder={}", time, ts);
        log.info("micro-batch stored in folder={}",ts);

            if(actenaissancesJavaRDD.isEmpty()){
                log.info("no data found!");
                return;
            }

            log.info("data under processing...");

            Dataset<ActeNaissances> actenaissancesDataset = sparkSession.createDataset(
                    actenaissancesJavaRDD.rdd(),
                    Encoders.bean(ActeNaissances.class)
            ).cache();


            actenaissancesDataset.printSchema();
            actenaissancesDataset.show(5, false);

            log.info("nb actesnaissances = {}", actenaissancesDataset.count());


            ActeNaissancesWriter<ActeNaissances> writer = new ActeNaissancesWriter<>(outputPathStr + "/time=" + ts);
            writer.accept(actenaissancesDataset);

            actenaissancesDataset.unpersist();
            log.info("done");
        }
}
